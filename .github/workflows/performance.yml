name: Performance Regression Detection

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run daily at 2 AM UTC to catch performance drift
  - cron: 0 2 * * *

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: uv-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

    - name: Install dependencies
      run: |
        uv sync --dev
        uv pip install pytest-benchmark

    - name: Run performance benchmarks
      run: |
        uv run pytest tests/test_performance_benchmarks.py \
          --benchmark-json=benchmark-results.json \
          --benchmark-warmup=on \
          --benchmark-warmup-iterations=2 \
          --benchmark-min-rounds=5 \
          -v

    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name != 'pull_request'
      with:
        tool: pytest
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # Show alert with commit comment on detecting possible performance regression
        alert-threshold: 150%
        comment-on-alert: true
        fail-on-alert: true
        benchmark-data-dir-path: benchmarks

    - name: Run performance regression detection
      run: |
        uv run python scripts/performance_monitor.py --ci-mode
      env:
        CI: true

    - name: Upload performance reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-reports-py${{ matrix.python-version }}
        path: |
          performance_data/*.md
          performance_data/*.json
          benchmark-results.json

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Read latest performance report
          const perfDataDir = 'performance_data';
          const reportFiles = fs.readdirSync(perfDataDir)
            .filter(f => f.startsWith('perf_report_') && f.endsWith('.md'))
            .sort()
            .reverse();

          if (reportFiles.length > 0) {
            const reportPath = path.join(perfDataDir, reportFiles[0]);
            const reportContent = fs.readFileSync(reportPath, 'utf8');

            const comment = `## 🚀 Performance Test Results

            <details>
            <summary>Click to expand performance report</summary>

            ${reportContent}

            </details>

            Benchmark results are available in the workflow artifacts.`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  import-speed-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install package
      run: |
        uv sync

    - name: 'Test import speed (target: <50ms)'
      run: |
        uv run python -c "
        import time
        import subprocess
        import sys

        times = []
        for _ in range(5):
            start = time.perf_counter()
            result = subprocess.run([sys.executable, '-c', 'import vexy_markliff'],
                                  capture_output=True)
            end = time.perf_counter()
            if result.returncode == 0:
                times.append(end - start)

        if times:
            avg_time = sum(times) / len(times)
            print(f'Average import time: {avg_time*1000:.1f}ms')

            # Check against target (50ms, but ideally <20ms)
            if avg_time > 0.050:
                print(f'❌ Import speed regression: {avg_time*1000:.1f}ms > 50ms target')
                sys.exit(1)
            elif avg_time < 0.020:
                print(f'✅ Excellent import speed: {avg_time*1000:.1f}ms < 20ms')
            else:
                print(f'✅ Good import speed: {avg_time*1000:.1f}ms < 50ms')
        else:
            print('❌ Import failed')
            sys.exit(1)
        "

  memory-efficiency-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install UV and dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
        uv sync --dev
        uv pip install psutil

    - name: Test memory efficiency
      run: |
        uv run python -c "
        import psutil
        import os
        from vexy_markliff import VexyMarkliff

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024

        # Test conversion memory usage
        converter = VexyMarkliff()
        after_init = process.memory_info().rss / 1024 / 1024

        # Convert large document
        large_content = '# Test Document\n\n' + ('Test paragraph content. ' * 100) * 50
        result = converter.markdown_to_xliff(large_content, 'en', 'es')
        after_conversion = process.memory_info().rss / 1024 / 1024

        init_overhead = after_init - initial_memory
        conversion_overhead = after_conversion - after_init

        print(f'Initial memory: {initial_memory:.1f}MB')
        print(f'After init: {after_init:.1f}MB (+{init_overhead:.1f}MB)')
        print(f'After conversion: {after_conversion:.1f}MB (+{conversion_overhead:.1f}MB)')

        # Check memory usage is reasonable
        if conversion_overhead > 15:
            print(f'❌ Memory usage regression: {conversion_overhead:.1f}MB > 15MB threshold')
            exit(1)
        else:
            print(f'✅ Good memory efficiency: {conversion_overhead:.1f}MB overhead')
        "

  throughput-benchmark:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install dependencies
      run: uv sync

    - name: Test conversion throughput
      run: |
        uv run python -c "
        import time
        from vexy_markliff import VexyMarkliff

        converter = VexyMarkliff()

        # Generate medium-sized test document
        sections = []
        for i in range(30):
            sections.append(f'''
        ## Section {i+1}

        Content with **bold** and *italic* text and `code`.

        - List item {i+1}-1
        - List item {i+1}-2
        - List item {i+1}-3

        ```python
        def function_{i}():
            return {i}
        ```
        ''')

        content = '# Throughput Test\n' + '\n'.join(sections)
        content_size_kb = len(content.encode('utf-8')) / 1024

        # Measure conversion throughput
        times = []
        for _ in range(3):
            start = time.perf_counter()
            result = converter.markdown_to_xliff(content, 'en', 'es')
            end = time.perf_counter()
            times.append(end - start)

        avg_time = sum(times) / len(times)
        throughput = content_size_kb / avg_time

        print(f'Document size: {content_size_kb:.1f}KB')
        print(f'Average conversion time: {avg_time:.3f}s')
        print(f'Throughput: {throughput:.1f} KB/s')

        # Check throughput meets minimum requirement
        if throughput < 100:
            print(f'❌ Throughput regression: {throughput:.1f} KB/s < 100 KB/s minimum')
            exit(1)
        elif throughput > 500:
            print(f'✅ Excellent throughput: {throughput:.1f} KB/s')
        else:
            print(f'✅ Good throughput: {throughput:.1f} KB/s')
        "
